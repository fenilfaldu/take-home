#!/bin/bash

# Fix pandas memory optimization issues in the notebook
cat > /workspace/process_data.ipynb << 'EOF'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Data Processing (Memory Optimized)\n",
    "\n",
    "This notebook processes large sales CSV files with memory optimization using chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "def sum_sq(x):\n",
    "    return (x**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sales_data(input_file, output_dir):\n",
    "    \"\"\"Process large sales CSV file with memory optimization\"\"\"\n",
    "    \n",
    "    print(f\"Processing {input_file}...\")\n",
    "    \n",
    "    # Initialize accumulators for aggregations\n",
    "    daily_agg = defaultdict(lambda: {'revenue': 0, 'quantity': 0, 'profit_margin_sum': 0, 'count': 0})\n",
    "    monthly_agg = defaultdict(lambda: {'revenue': 0, 'quantity': 0, 'profit_margin_sum': 0, 'count': 0})\n",
    "    \n",
    "    category_agg = defaultdict(lambda: {\n",
    "        'revenue_sum': 0, 'revenue_sum_sq': 0, 'revenue_count': 0,\n",
    "        'quantity_sum': 0, 'quantity_sum_sq': 0, 'quantity_count': 0,\n",
    "        'profit_margin_sum': 0, 'profit_margin_sum_sq': 0, 'profit_margin_count': 0\n",
    "    })\n",
    "    \n",
    "    total_rows = 0\n",
    "    \n",
    "    # Pass 1: Generate per-year files (heavy memory usage due to all columns)\n",
    "    print(\"Pass 1: Generating per-year files...\")\n",
    "    # Use small chunk size for memory-constrained processing\n",
    "    chunk_size_pass1 = 1000 \n",
    "    year_files = {}\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size_pass1)):\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "        # Process string columns one by one to save memory\n",
    "        string_cols = chunk.select_dtypes(include=['object']).columns.tolist()\n",
    "        for col in string_cols:\n",
    "            # Create lower case version\n",
    "            lower_val = chunk[col].str.lower()\n",
    "            # Update inplace\n",
    "            chunk[col] = chunk[col].str.upper()\n",
    "            # Add new column\n",
    "            chunk[f'{col}_lower'] = lower_val\n",
    "            del lower_val\n",
    "        \n",
    "        # Parse dates and extract components\n",
    "        chunk['date'] = pd.to_datetime(chunk['date'])\n",
    "        chunk['year'] = chunk['date'].dt.year\n",
    "        chunk['month'] = chunk['date'].dt.month\n",
    "        chunk['day'] = chunk['date'].dt.day\n",
    "        chunk['weekday'] = chunk['date'].dt.dayofweek\n",
    "        chunk['quarter'] = chunk['date'].dt.quarter\n",
    "        \n",
    "        # Calculate revenue, profit_margin, and tax\n",
    "        chunk['revenue'] = chunk['quantity'] * chunk['price']\n",
    "        chunk['profit_margin'] = chunk['revenue'] * 0.3\n",
    "        chunk['tax'] = chunk['revenue'] * 0.1\n",
    "        \n",
    "        # Write per-year files in append mode\n",
    "        years = chunk['year'].unique()\n",
    "        for year in years:\n",
    "            # Use boolean indexing carefully\n",
    "            year_mask = chunk['year'] == year\n",
    "            year_data = chunk[year_mask]\n",
    "            \n",
    "            year_file = os.path.join(output_dir, f'data_{year}.csv')\n",
    "            \n",
    "            if year not in year_files:\n",
    "                year_data.to_csv(year_file, index=False, mode='w')\n",
    "                year_files[year] = True\n",
    "            else:\n",
    "                year_data.to_csv(year_file, index=False, mode='a', header=False)\n",
    "            \n",
    "            # Explicitly delete temporary dataframe\n",
    "            del year_data\n",
    "            del year_mask\n",
    "        \n",
    "        # Clear chunk immediately\n",
    "        del chunk\n",
    "        \n",
    "        # Force GC every chunk to stay under strict limit\n",
    "        gc.collect()\n",
    "            \n",
    "    # Pass 2: Aggregations (lighter memory usage, only needed columns)\n",
    "    print(\"Pass 2: Calculating aggregations...\")\n",
    "    chunk_size_pass2 = 2000\n",
    "    usecols = ['date', 'category', 'quantity', 'price']\n",
    "    \n",
    "    for chunk_num, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size_pass2, usecols=usecols)):\n",
    "        chunk['date'] = pd.to_datetime(chunk['date'])\n",
    "        chunk['year'] = chunk['date'].dt.year\n",
    "        chunk['month'] = chunk['date'].dt.month\n",
    "        \n",
    "        chunk['revenue'] = chunk['quantity'] * chunk['price']\n",
    "        chunk['profit_margin'] = chunk['revenue'] * 0.3\n",
    "        \n",
    "        # Daily aggregation\n",
    "        daily_groups = chunk.groupby(['date', 'category']).agg({\n",
    "            'revenue': 'sum',\n",
    "            'quantity': 'sum',\n",
    "            'profit_margin': 'sum'\n",
    "        })\n",
    "        for (date, cat), row in daily_groups.iterrows():\n",
    "            daily_key = (date, cat)\n",
    "            daily_agg[daily_key]['revenue'] += row['revenue']\n",
    "            daily_agg[daily_key]['quantity'] += row['quantity']\n",
    "            daily_agg[daily_key]['profit_margin_sum'] += row['profit_margin']\n",
    "        \n",
    "        # Count per group\n",
    "        daily_counts = chunk.groupby(['date', 'category']).size()\n",
    "        for (date, cat), count in daily_counts.items():\n",
    "            daily_agg[(date, cat)]['count'] += count\n",
    "        \n",
    "        # Monthly aggregation\n",
    "        monthly_groups = chunk.groupby(['year', 'month', 'category']).agg({\n",
    "            'revenue': 'sum',\n",
    "            'quantity': 'sum',\n",
    "            'profit_margin': 'sum'\n",
    "        })\n",
    "        for (year, month, cat), row in monthly_groups.iterrows():\n",
    "            monthly_key = (year, month, cat)\n",
    "            monthly_agg[monthly_key]['revenue'] += row['revenue']\n",
    "            monthly_agg[monthly_key]['quantity'] += row['quantity']\n",
    "            monthly_agg[monthly_key]['profit_margin_sum'] += row['profit_margin']\n",
    "        \n",
    "        monthly_counts = chunk.groupby(['year', 'month', 'category']).size()\n",
    "        for (year, month, cat), count in monthly_counts.items():\n",
    "            monthly_agg[(year, month, cat)]['count'] += count\n",
    "        \n",
    "        # Category aggregation\n",
    "        cat_groups = chunk.groupby('category').agg({\n",
    "            'revenue': ['sum', sum_sq, 'count'],\n",
    "            'quantity': ['sum', sum_sq, 'count'],\n",
    "            'profit_margin': ['sum', sum_sq, 'count']\n",
    "        })\n",
    "        \n",
    "        for cat in cat_groups.index:\n",
    "            category_agg[cat]['revenue_sum'] += cat_groups.loc[cat, ('revenue', 'sum')]\n",
    "            category_agg[cat]['revenue_sum_sq'] += cat_groups.loc[cat, ('revenue', 'sum_sq')]\n",
    "            category_agg[cat]['revenue_count'] += cat_groups.loc[cat, ('revenue', 'count')]\n",
    "            \n",
    "            category_agg[cat]['quantity_sum'] += cat_groups.loc[cat, ('quantity', 'sum')]\n",
    "            category_agg[cat]['quantity_sum_sq'] += cat_groups.loc[cat, ('quantity', 'sum_sq')]\n",
    "            category_agg[cat]['quantity_count'] += cat_groups.loc[cat, ('quantity', 'count')]\n",
    "            \n",
    "            category_agg[cat]['profit_margin_sum'] += cat_groups.loc[cat, ('profit_margin', 'sum')]\n",
    "            category_agg[cat]['profit_margin_sum_sq'] += cat_groups.loc[cat, ('profit_margin', 'sum_sq')]\n",
    "            category_agg[cat]['profit_margin_count'] += cat_groups.loc[cat, ('profit_margin', 'count')]\n",
    "        \n",
    "        del chunk, daily_groups, monthly_groups, cat_groups, daily_counts, monthly_counts\n",
    "        if chunk_num % 5 == 0:\n",
    "            gc.collect()\n",
    "    \n",
    "    print(f\"Loaded {total_rows} rows\")\n",
    "    print(\"Writing output files...\")\n",
    "    \n",
    "    # Build daily summary from aggregated data\n",
    "    daily_data = []\n",
    "    for (date, category), agg in daily_agg.items():\n",
    "        daily_data.append({\n",
    "            'date': date,\n",
    "            'category': category,\n",
    "            'revenue': agg['revenue'],\n",
    "            'quantity': agg['quantity'],\n",
    "            'profit_margin': agg['profit_margin_sum'] / agg['count']\n",
    "        })\n",
    "    daily_summary = pd.DataFrame(daily_data)\n",
    "    daily_summary = daily_summary.sort_values(['date', 'category']).reset_index(drop=True)\n",
    "    daily_summary.to_csv(os.path.join(output_dir, 'daily_summary.csv'), index=False)\n",
    "    \n",
    "    # Build monthly summary from aggregated data\n",
    "    monthly_data = []\n",
    "    for (year, month, category), agg in monthly_agg.items():\n",
    "        monthly_data.append({\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'category': category,\n",
    "            'revenue': agg['revenue'],\n",
    "            'quantity': agg['quantity'],\n",
    "            'profit_margin': agg['profit_margin_sum'] / agg['count']\n",
    "        })\n",
    "    monthly_summary = pd.DataFrame(monthly_data)\n",
    "    monthly_summary = monthly_summary.sort_values(['year', 'month', 'category']).reset_index(drop=True)\n",
    "    monthly_summary.to_csv(os.path.join(output_dir, 'monthly_summary.csv'), index=False)\n",
    "    \n",
    "    # Build category summary with statistics using variance formula\n",
    "    category_data = []\n",
    "    for category, agg in category_agg.items():\n",
    "        # Calculate mean\n",
    "        revenue_mean = agg['revenue_sum'] / agg['revenue_count']\n",
    "        quantity_mean = agg['quantity_sum'] / agg['quantity_count']\n",
    "        profit_margin_mean = agg['profit_margin_sum'] / agg['profit_margin_count']\n",
    "        \n",
    "        # Calculate variance: Var(X) = E[X^2] - (E[X])^2\n",
    "        revenue_var = (agg['revenue_sum_sq'] / agg['revenue_count']) - (revenue_mean ** 2)\n",
    "        quantity_var = (agg['quantity_sum_sq'] / agg['quantity_count']) - (quantity_mean ** 2)\n",
    "        profit_margin_var = (agg['profit_margin_sum_sq'] / agg['profit_margin_count']) - (profit_margin_mean ** 2)\n",
    "        \n",
    "        # Calculate std (handle negative variance due to floating point errors)\n",
    "        revenue_std = np.sqrt(max(0, revenue_var))\n",
    "        quantity_std = np.sqrt(max(0, quantity_var))\n",
    "        profit_margin_std = np.sqrt(max(0, profit_margin_var))\n",
    "        \n",
    "        category_data.append({\n",
    "            'category': category,\n",
    "            'revenue_sum': agg['revenue_sum'],\n",
    "            'revenue_mean': revenue_mean,\n",
    "            'revenue_std': revenue_std,\n",
    "            'quantity_sum': agg['quantity_sum'],\n",
    "            'quantity_mean': quantity_mean,\n",
    "            'quantity_std': quantity_std,\n",
    "            'profit_margin_mean': profit_margin_mean,\n",
    "            'profit_margin_std': profit_margin_std\n",
    "        })\n",
    "    \n",
    "    # Create multi-index columns to match original format\n",
    "    category_summary = pd.DataFrame(category_data)\n",
    "    category_summary = category_summary.sort_values('category').reset_index(drop=True)\n",
    "    \n",
    "    # Reshape to multi-level columns\n",
    "    cat_multi = pd.DataFrame({\n",
    "        'category': category_summary['category'],\n",
    "        ('revenue', 'sum'): category_summary['revenue_sum'],\n",
    "        ('revenue', 'mean'): category_summary['revenue_mean'],\n",
    "        ('revenue', 'std'): category_summary['revenue_std'],\n",
    "        ('quantity', 'sum'): category_summary['quantity_sum'],\n",
    "        ('quantity', 'mean'): category_summary['quantity_mean'],\n",
    "        ('quantity', 'std'): category_summary['quantity_std'],\n",
    "        ('profit_margin', 'mean'): category_summary['profit_margin_mean'],\n",
    "        ('profit_margin', 'std'): category_summary['profit_margin_std']\n",
    "    })\n",
    "    cat_multi.to_csv(os.path.join(output_dir, 'category_summary.csv'), index=False)\n",
    "    \n",
    "    # Create pivot tables from aggregated daily summary\n",
    "    pivot_daily = daily_summary.pivot_table(\n",
    "        values='revenue',\n",
    "        index='date',\n",
    "        columns='category',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    pivot_daily.to_csv(os.path.join(output_dir, 'pivot_daily.csv'))\n",
    "    \n",
    "    # Create pivot table from monthly summary\n",
    "    pivot_monthly = monthly_summary.pivot_table(\n",
    "        values='revenue',\n",
    "        index=['year', 'month'],\n",
    "        columns='category',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    pivot_monthly.to_csv(os.path.join(output_dir, 'pivot_monthly.csv'))\n",
    "    \n",
    "    print(f\"Processing complete. Output files saved to {output_dir}\")\n",
    "    \n",
    "    # Calculate summary statistics for return value\n",
    "    total_revenue = sum(agg['revenue'] for agg in daily_agg.values())\n",
    "    unique_categories = len(category_agg)\n",
    "    dates = [key[0] for key in daily_agg.keys()]\n",
    "    min_date = min(dates)\n",
    "    max_date = max(dates)\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'total_revenue': total_revenue,\n",
    "        'unique_categories': unique_categories,\n",
    "        'date_range': f\"{min_date} to {max_date}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "# Process the sales data\n",
    "results = process_sales_data('data/sales_data.csv', 'output')\n",
    "print(f\"Results: {results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF

# Verify the notebook exists
if [ -f /workspace/process_data.ipynb ]; then
    echo "Notebook optimized successfully"
else
    echo "Error: Failed to create notebook"
    exit 1
fi
